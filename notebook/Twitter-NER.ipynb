{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Baseline Named Entity Recognizer for Twitter\n",
    "\n",
    "In this notebook I'll follow the example presented in [Named entities and random fields](http://www.orbifold.net/default/2017/06/29/dutch-ner/) to train a conditional random field to recognize named entities in Twitter data. The data and some of the code below are taken from a programming assignment in the amazing class [Natural Language Processing](https://www.coursera.org/learn/language-processing) offered by [Coursera](https://www.coursera.org/). In the assignment we were shown how to build a named entity recognizer using deep learning with a bidirectional LSTM, which is a pretty complicated approach and I wanted to have a baseline model to see what sort of accuracy should be expected on this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preparing the Data\n",
    "\n",
    "First load the text and tags for training, validation and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    \n",
    "    tweet_tokens = []\n",
    "    tweet_tags = []\n",
    "    for line in open(file_path, encoding='utf-8'):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            if tweet_tokens:\n",
    "                tokens.append(tweet_tokens)\n",
    "                tags.append(tweet_tags)\n",
    "            tweet_tokens = []\n",
    "            tweet_tags = []\n",
    "        else:\n",
    "            token, tag = line.split()\n",
    "            # Replace all urls with <URL> token\n",
    "            # Replace all users with <USR> token\n",
    "            if token.startswith(\"http://\") or token.startswith(\"https://\"): token = \"<URL>\"\n",
    "            elif token.startswith(\"@\"): token = \"<USR>\"\n",
    "            tweet_tokens.append(token)\n",
    "            tweet_tags.append(tag) \n",
    "    return tokens, tags\n",
    "train_tokens, train_tags = read_data('data/train.txt')\n",
    "validation_tokens, validation_tags = read_data('data/validation.txt')\n",
    "test_tokens, test_tags = read_data('data/test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CRF model uses part of speech tags as features so we'll neet to add those to the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.06 s, sys: 192 ms, total: 7.26 s\n",
      "Wall time: 7.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import nltk\n",
    "\n",
    "def build_sentence(tokens, tags):\n",
    "    pos_tags = [item[-1] for item in nltk.pos_tag(tokens)]\n",
    "    return list(zip(tokens, pos_tags, tags))\n",
    "\n",
    "def build_sentences(tokens_set, tags_set):\n",
    "    return [build_sentence(tokens, tags) for tokens, tags in zip(tokens_set, tags_set)]\n",
    "\n",
    "train_sents = build_sentences(train_tokens, train_tags)\n",
    "validation_sents = build_sentences(validation_tokens, validation_tags)\n",
    "test_sents = build_sentences(test_tokens, test_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Computing Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i - 1][0]\n",
    "        postag1 = sent[i - 1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent) - 1:\n",
    "        word1 = sent[i + 1][0]\n",
    "        postag1 = sent[i + 1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]\n",
    "\n",
    "\n",
    "X_train = [sent2features(s) for s in train_sents]\n",
    "y_train = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "X_validation = [sent2features(s) for s in validation_sents]\n",
    "y_validation = [sent2labels(s) for s in validation_sents]\n",
    "\n",
    "X_test = [sent2features(s) for s in test_sents]\n",
    "y_test = [sent2labels(s) for s in test_sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn_crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_states=None,\n",
       "  all_possible_transitions=True, averaging=None, c=None, c1=0.12, c2=0.01,\n",
       "  calibration_candidates=None, calibration_eta=None,\n",
       "  calibration_max_trials=None, calibration_rate=None,\n",
       "  calibration_samples=None, delta=None, epsilon=None, error_sensitive=None,\n",
       "  gamma=None, keep_tempfiles=None, linesearch=None, max_iterations=100,\n",
       "  max_linesearch=None, min_freq=None, model_filename=None,\n",
       "  num_memories=None, pa_type=None, period=None, trainer_cls=None,\n",
       "  variance=None, verbose=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.12,\n",
    "    c2=0.01,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluate the Model\n",
    "\n",
    "We evaluate the model using the CoNLL shared task evaluation script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from evaluation import precision_recall_f1\n",
    "\n",
    "def eval_conll(model, tokens, tags, short_report=True):\n",
    "    \"\"\"Computes NER quality measures using CONLL shared task script.\"\"\"\n",
    "    tags_pred = model.predict(tokens)\n",
    "    y_true = [y for s in tags for y in s] \n",
    "    y_pred = [y for s in tags_pred for y in s] \n",
    "    results = precision_recall_f1(y_true, y_pred, print_results=True, short_report=short_report)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Train set quality: --------------------\n",
      "processed 99983 tokens with 4489 phrases; found: 4476 phrases; correct: 4433.\n",
      "\n",
      "precision:  99.04%; recall:  98.75%; F1:  98.90\n",
      "\n",
      "\t     company: precision:   98.75%; recall:   98.13%; F1:   98.44; predicted:   639\n",
      "\n",
      "\t    facility: precision:   97.76%; recall:   97.13%; F1:   97.44; predicted:   312\n",
      "\n",
      "\t     geo-loc: precision:   99.20%; recall:   99.40%; F1:   99.30; predicted:   998\n",
      "\n",
      "\t       movie: precision:  100.00%; recall:  100.00%; F1:  100.00; predicted:    68\n",
      "\n",
      "\t musicartist: precision:   97.85%; recall:   98.28%; F1:   98.06; predicted:   233\n",
      "\n",
      "\t       other: precision:   98.94%; recall:   98.68%; F1:   98.81; predicted:   755\n",
      "\n",
      "\t      person: precision:   99.32%; recall:   98.98%; F1:   99.15; predicted:   883\n",
      "\n",
      "\t     product: precision:   99.68%; recall:   99.06%; F1:   99.37; predicted:   316\n",
      "\n",
      "\t  sportsteam: precision:  100.00%; recall:   99.54%; F1:   99.77; predicted:   216\n",
      "\n",
      "\t      tvshow: precision:  100.00%; recall:   96.55%; F1:   98.25; predicted:    56\n",
      "\n",
      "-------------------- Validation set quality: --------------------\n",
      "processed 12112 tokens with 537 phrases; found: 317 phrases; correct: 213.\n",
      "\n",
      "precision:  67.19%; recall:  39.66%; F1:  49.88\n",
      "\n",
      "\t     company: precision:   78.67%; recall:   56.73%; F1:   65.92; predicted:    75\n",
      "\n",
      "\t    facility: precision:   76.92%; recall:   29.41%; F1:   42.55; predicted:    13\n",
      "\n",
      "\t     geo-loc: precision:   76.25%; recall:   53.98%; F1:   63.21; predicted:    80\n",
      "\n",
      "\t       movie: precision:  100.00%; recall:   14.29%; F1:   25.00; predicted:     1\n",
      "\n",
      "\t musicartist: precision:   55.56%; recall:   17.86%; F1:   27.03; predicted:     9\n",
      "\n",
      "\t       other: precision:   52.17%; recall:   29.63%; F1:   37.80; predicted:    46\n",
      "\n",
      "\t      person: precision:   67.12%; recall:   43.75%; F1:   52.97; predicted:    73\n",
      "\n",
      "\t     product: precision:   14.29%; recall:    5.88%; F1:    8.33; predicted:    14\n",
      "\n",
      "\t  sportsteam: precision:   33.33%; recall:   10.00%; F1:   15.38; predicted:     6\n",
      "\n",
      "\t      tvshow: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
      "\n",
      "-------------------- Test set quality: --------------------\n",
      "processed 12534 tokens with 604 phrases; found: 383 phrases; correct: 273.\n",
      "\n",
      "precision:  71.28%; recall:  45.20%; F1:  55.32\n",
      "\n",
      "\t     company: precision:   85.71%; recall:   57.14%; F1:   68.57; predicted:    56\n",
      "\n",
      "\t    facility: precision:   72.73%; recall:   51.06%; F1:   60.00; predicted:    33\n",
      "\n",
      "\t     geo-loc: precision:   82.91%; recall:   58.79%; F1:   68.79; predicted:   117\n",
      "\n",
      "\t       movie: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     2\n",
      "\n",
      "\t musicartist: precision:   33.33%; recall:    7.41%; F1:   12.12; predicted:     6\n",
      "\n",
      "\t       other: precision:   55.07%; recall:   36.89%; F1:   44.19; predicted:    69\n",
      "\n",
      "\t      person: precision:   64.20%; recall:   50.00%; F1:   56.22; predicted:    81\n",
      "\n",
      "\t     product: precision:   37.50%; recall:   10.71%; F1:   16.67; predicted:     8\n",
      "\n",
      "\t  sportsteam: precision:   81.82%; recall:   29.03%; F1:   42.86; predicted:    11\n",
      "\n",
      "\t      tvshow: precision:    0.00%; recall:    0.00%; F1:    0.00; predicted:     0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('-' * 20 + ' Train set quality: ' + '-' * 20)\n",
    "train_results = eval_conll(crf, X_train, y_train, short_report=False)\n",
    "\n",
    "print('-' * 20 + ' Validation set quality: ' + '-' * 20)\n",
    "validation_results = eval_conll(crf, X_validation, y_validation, short_report=False)\n",
    "\n",
    "print('-' * 20 + ' Test set quality: ' + '-' * 20)\n",
    "test_results = eval_conll(crf, X_test, y_test, short_report=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Tuning Parameters\n",
    "\n",
    "I tried tuning the parameters c1 and c2 of the model using [randomized grid search](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) but was not able to improve the results that way.  I plan to try [GPyOpt](https://github.com/SheffieldML/GPyOpt) to see if that will do better but don't have time to do that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
